{"introduction-end-analytics-use-microsoft-fabric": [{"Q": "What is Microsoft Fabric?", "A": "Microsoft Fabric is a unified end-to-end analytics platform, providing a single, integrated environment for data professionals and the business to collaborate on data projects.", "source": "1-introduction"}, {"Q": "What are the services included in Microsoft Fabric?", "A": "The services include data engineering, data integration, data warehousing, real-time analytics, data science, and business intelligence.", "source": "1-introduction"}, {"Q": "What is OneLake?", "A": "OneLake is Fabric's lake-centric architecture that provides a single, integrated environment for data professionals and the business to collaborate on data projects.", "source": "2-explore-analytics-fabric"}, {"Q": "What formats can be stored in OneLake?", "A": "Data can be stored in any format, including Delta, Parquet, CSV, JSON, and more.", "source": "2-explore-analytics-fabric"}, {"Q": "What are some of the analytics experiences offered by Fabric?", "A": "Synapse Data Engineering, Synapse Data Warehouse, Synapse Data Science, Real-Time Intelligence, Data Factory, and Power BI.", "source": "2-explore-analytics-fabric"}], "use-data-factory-pipelines-fabric": [{"Q": "What's the purpose of data pipelines?", "A": "To extract, transform, and load data from one or more sources to a destination.", "source": "1-introduction"}, {"Q": "What's the role of pipelines in ETL processes?", "A": "To automate the ingestion of transactional data into an analytical data store.", "source": "1-introduction"}, {"Q": "What information is available in the pipeline run history?", "A": "You can see start and end times, duration, how each run was triggered, and current status.", "source": "5-run-monitor-pipelines"}, {"Q": "How can pipelines be executed in Microsoft Fabric?", "A": "Pipelines can be created interactively in the user interface or scheduled to run automatically.", "source": "1-introduction"}, {"Q": "When should you use the Copy Data activity?", "A": "When you need to copy data directly without transformations, or when you want to import raw data and apply transformations in later pipeline activities.", "source": "3-copy-data"}, {"Q": "Where can you view the run history for a pipeline?", "A": "You can view it from the pipeline canvas or from the pipeline item listed on the workspace page.", "source": "5-run-monitor-pipelines"}, {"Q": "What is the purpose of parameters in a pipeline?", "A": "Parameters enable you to provide specific values to be used each time a pipeline is run, increasing the reusability of your pipelines.", "source": "2-understand-fabric-pipeline"}, {"Q": "What is a pipeline run?", "A": "A pipeline run is initiated each time a pipeline is executed, and can be reviewed to confirm completion and investigate settings used.", "source": "2-understand-fabric-pipeline"}], "use-apache-spark-work-files-lakehouse": [{"Q": "What are the two types of content that cells in a notebook can contain?", "A": "Markdown-formatted content or executable code.", "source": "3-spark-code"}, {"Q": "What is the most commonly used data structure for working with structured data in Spark?", "A": "Dataframe", "source": "4-dataframe"}, {"Q": "How can you save a dataframe as a partitioned set of files?", "A": "By using the 'partitionBy' method when writing the data.", "source": "4-dataframe"}, {"Q": "What is the purpose of the Spark catalog?", "A": "The Spark catalog is a metastore for relational data objects such as views and tables.", "source": "5-spark-sql"}, {"Q": "How can you make data in a dataframe available for querying in the Spark catalog?", "A": "One way is to create a temporary view with the 'createOrReplaceTempView' method.", "source": "5-spark-sql"}, {"Q": "What is Apache Spark?", "A": "Apache Spark is an open-source parallel processing framework for large-scale data processing and analytics.", "source": "1-introduction"}, {"Q": "Where can I use Spark?", "A": "Spark is available in multiple platform implementations, including Azure HDInsight, Azure Databricks, and Microsoft Fabric.", "source": "1-introduction"}, {"Q": "What is the difference between a temporary view and a table in the Spark catalog?", "A": "A temporary view is automatically deleted at the end of the session, while a table is persistent in the catalog and can be queried using Spark SQL.", "source": "5-spark-sql"}, {"Q": "How does Spark process large volumes of data quickly?", "A": "Spark uses a \"divide and conquer\" approach by distributing the work across multiple systems.", "source": "2-spark"}, {"Q": "What languages can be used with Spark?", "A": "Java, Scala, Spark R, Spark SQL, and PySpark.", "source": "2-spark"}, {"Q": "What's the preferred format for tables in Microsoft Fabric?", "A": "The preferred format is delta, which is the format for a relational data technology on Spark named Delta Lake.", "source": "5-spark-sql"}], "get-started-data-warehouse": [{"Q": "What's the standard design for a relational data warehouse?", "A": "A common pattern is to use a denormalized, multidimensional schema.", "source": "1-introduction"}, {"Q": "What's unique about Microsoft Fabric's data warehouse?", "A": "It's built on the Lakehouse, which is stored in Delta format and can be queried using SQL.", "source": "1-introduction"}, {"Q": "What are the benefits of using Fabric's data warehouse?", "A": "It enables data engineers, data scientists, and business analysts to work together to create and query a data warehouse that is optimized for their specific needs.", "source": "1-introduction"}, {"Q": "What are the steps involved in building a modern data warehouse?", "A": "Data ingestion, data storage, data processing, data analysis, and insights delivery.", "source": "2-understand-data-warehouse"}, {"Q": "What are dimension tables in a data warehouse?", "A": "Dimension tables contain descriptive information about the data in fact tables.", "source": "2-understand-data-warehouse"}, {"Q": "What's the purpose of surrogate and alternate keys in a data warehouse?", "A": "Surrogate keys help maintain consistency and accuracy, while alternate keys help maintain traceability between the data warehouse and the source system.", "source": "2-understand-data-warehouse"}, {"Q": "What are fact tables in a data warehouse?", "A": "Fact tables contain numerical data that you want to analyze.", "source": "2-understand-data-warehouse"}, {"Q": "What can you do in the Visual query editor?", "A": "You can drag a table to the canvas, add columns, filters, and other transformations to the query", "source": "4-query-transform-data"}, {"Q": "What is a data model?", "A": "A data model defines the relationships between tables, rules for data aggregation, and measures used to derive insights.", "source": "5-model-data"}, {"Q": "What is the purpose of workspace roles in securing data warehouse?", "A": "Workspace roles are used to control access and manage the lifecycle of data and services in Fabric.", "source": "6-security-monitor"}], "use-dataflow-gen-2-fabric": [{"Q": "What's the purpose of Dataflows (Gen2)?", "A": "Dataflows (Gen2) are used to ingest and transform data from multiple sources, and then land the cleansed data in another destination.", "source": "1-introduction"}, {"Q": "Why are dataflows important in end-to-end analytics?", "A": "Dataflows (Gen2) enable you to prepare data to ensure consistency, stage data in your preferred destination, reuse data, and easily update data.", "source": "1-introduction"}, {"Q": "What are Dataflows (Gen2)?", "A": "They are a type of cloud-based ETL tool for building and executing scalable data transformation processes.", "source": "2-dataflows-gen-2"}, {"Q": "What are the benefits of using Dataflows (Gen2)?", "A": "Extend data with consistent data, allow self-service users access to a subset of data, optimize performance, simplify data source complexity, ensure data consistency and quality, and simplify data integration.", "source": "2-dataflows-gen-2"}, {"Q": "What are some common data transformations in Dataflows (Gen2)?", "A": "\u2022 Filter and Sort rows\n\u2022 Pivot and Unpivot\n\u2022 Merge and Append queries\n\u2022 Split and Conditional split\n\u2022 Replace values and Remove duplicates\n\u2022 Add, Rename, Reorder, or Delete columns\n\u2022 Rank and Percentage calculator\n\u2022 Top N and Bottom N", "source": "3-explore-dataflows-gen-2"}, {"Q": "How can dataflows be incorporated into pipelines?", "A": "Dataflows can be incorporated into a pipeline to orchestrate extra activities, like executing scripts or stored procedures after the dataflow has completed", "source": "4-dataflow-pipeline"}, {"Q": "What does the Diagram View in Power Query Online allow you to do?", "A": "It allows you to visually see how the data sources are connected and the applied transformations.", "source": "3-explore-dataflows-gen-2"}], "work-delta-lake-tables-fabric": [{"Q": "What's Delta Lake?", "A": "Delta Lake is an open-source storage layer for Spark that enables relational database capabilities for batch and streaming data.", "source": "1-introduction"}, {"Q": "What's the advantage of using Delta Lake in a lakehouse architecture?", "A": "Delta Lake offers the benefits of a relational database system with the flexibility of storing data in a data lake.", "source": "1-introduction"}, {"Q": "Do you need to work directly with Delta Lake APIs to use tables in a Fabric lakehouse?", "A": "No, you can also use SQL-based data manipulation.", "source": "1-introduction"}, {"Q": "What are the benefits of using Delta tables?", "A": "Delta tables support:\n\u2022 querying and data modification\n\u2022 ACID transactions\n\u2022 versioning and time trave\n\u2022 batch and streaming datal\n\u2022 standard formats and interoperability", "source": "2-understand-delta-lake"}, {"Q": "What's the difference between managed and external tables?", "A": "Managed tables have both table definition and data files managed by the Spark runtime for the Fabric lakehouse, while external tables have the table definition mapped to an alternative file storage location.", "source": "3-create-delta-tables"}, {"Q": "What's the advantage of saving data in delta format without creating a table definition?", "A": "It allows you to persist the results of data transformations and later overlay a table definition or process directly using the delta lake API.", "source": "3-create-delta-tables"}, {"Q": "What is stored in the _delta_Log folder for each table?", "A": "Transaction details are logged in JSON format in the _delta_Log folder.", "source": "2-understand-delta-lake"}, {"Q": "What format is the underlying data for delta tables stored in?", "A": "The underlying data is stored in Parquet format.", "source": "2-understand-delta-lake"}, {"Q": "How can you retrieve older versions of the data in a delta table?", "A": "Use logged transactions to view the history of changes and then specify the version or timestamp to retrieve older versions of the data (known as time travel).", "source": "4-work-delta-data"}, {"Q": "What are the types of streaming sources that Spark Structured Streaming can read from?", "A": "Spark Structured Streaming can read data from network ports, real time message brokering services, or file system locations.", "source": "5-use-delta-lake-streaming-data"}, {"Q": "What is the purpose of using a delta table as a streaming source?", "A": "Using a delta table as a streaming source allows you to constantly report new data as it is added to the table.", "source": "5-use-delta-lake-streaming-data"}, {"Q": "What operations can be included in a delta table used as a streaming source?", "A": "When using a delta table as a streaming source, only append operations can be included in the stream.", "source": "5-use-delta-lake-streaming-data"}, {"Q": "What is the purpose of the checkpointLocation option when writing a stream to a delta table?", "A": "The checkpointLocation option is used to write a checkpoint file that tracks the state of the stream processing.", "source": "5-use-delta-lake-streaming-data"}], "explore-event-streams-microsoft-fabric": [{"Q": "What is Microsoft Fabric event stream?", "A": "A feature that enables you to handle real-time events without coding.", "source": "1-introduction"}, {"Q": "What are some tasks you can perform using event stream?", "A": "You can set up event sources, destinations, and processors, and perform event processing for collecting and aggregating data.", "source": "1-introduction"}, {"Q": "What's the event stream main editor used for?", "A": "Establishing sources and destinations, viewing data in-flight, and capturing, transforming, and routing data.", "source": "1-introduction"}, {"Q": "How does event stream work?", "A": "Event stream works by creating a pipeline of events from multiple sources to different destinations.", "source": "2-eventstream-components"}, {"Q": "What are the main components of event stream?", "A": "\u2022 Event stream\n\u2022 Source\n\u2022 Destination\n\u2022 Main Editor.", "source": "2-eventstream-components"}, {"Q": "What are the available event sources in event stream?", "A": "The available event sources in event stream are Azure Event Hubs, sample data, and custom app.", "source": "3-setup-eventstreams"}, {"Q": "What types of event destinations are available in event stream?", "A": "KQL database, lakehouse, custom app, and Reflex.", "source": "4-route-event-data-to-destinations"}, {"Q": "What are some event processor operations available in event stream?", "A": "Some event processor operations available in event stream are Aggregate, Expand, Filter, Group by, Manage fields, and Union.", "source": "4-route-event-data-to-destinations"}], "administer-fabric": [{"Q": "What do Fabric admins need to have a solid understanding of?", "A": "Fabric architecture, security and governance features, analytics capabilities, and the various deployment and licensing options available.", "source": "1-introduction"}, {"Q": "What's the role of a Fabric admin?", "A": "To ensure that Fabric is deployed and used in a way that meets business objectives and complies with organizational policies and standards.", "source": "1-introduction"}, {"Q": "What's a Fabric tenant?", "A": "A tenant is a dedicated space for organizations to create, store, and manage Fabric items. It maps to the root of OneLake and is at the top level of the hierarchy.", "source": "2-fabric-architecture"}, {"Q": "What's a workspace in Fabric?", "A": "A workspace is a collection of items in Fabric that brings together different functionality in a single tenant. It acts as a container that leverages capacity for the work that is executed and provides controls for who can access the items in it.", "source": "2-fabric-architecture"}, {"Q": "What are some administrative tasks in Fabric?", "A": "Security and access control, data governance, customization and configuration, monitoring, and optimization.", "source": "3-admin-role-tools"}, {"Q": "What can you do in the Fabric admin portal?", "A": "\u2022 Manage settings by capacity or for the entire tenant\n\u2022 Manage users and groups\n\u2022 Access audit logs\n\u2022 Monitor usage and performance", "source": "3-admin-role-tools"}, {"Q": "What is the scanner API used for?", "A": "The scanner API is used to scan Fabric items for sensitive data.", "source": "5-govern-fabric"}], "ingest-data-with-spark-fabric-notebooks": [{"Q": "What are Fabric notebooks best suited for?", "A": "Handling large external datasets and performing complex transformations.", "source": "1-introduction"}, {"Q": "Where are Fabric notebooks stored?", "A": "In the workspace where they're created.", "source": "2-connect-authenticate"}, {"Q": "What languages can be used in Fabric notebooks?", "A": "PySpark, Html, Spark (Scala), Spark SQL, and SparkR (R).", "source": "2-connect-authenticate"}, {"Q": "What's the default engine used in Fabric notebooks?", "A": "PySpark, which uses the Spark engine.", "source": "2-connect-authenticate"}, {"Q": "What's the preferred format for saving data in a lakehouse?", "A": "Delta Parquet is the preferred format due to its optimized columnar storage structure and compression capabilities.", "source": "3-write-optimize"}, {"Q": "What are some optimization functions for Delta table writes?", "A": "V-Order and Optimize write are optimization functions that can be used to improve Delta table read and write performance.", "source": "3-write-optimize"}, {"Q": "What are some basic cleaning steps when loading data to ensure data quality and consistency?", "A": "Removing duplicates, handling errors, converting null values, and getting rid of empty entries.", "source": "4-considerations"}], "get-started-lakehouses": [{"Q": "What is a lakehouse?", "A": "A unified platform that combines the flexible and scalable storage of a data lake and the ability to query and analyze data in a data warehouse.", "source": "1-introduction"}, {"Q": "What are some examples of unstructured data?", "A": "Social media, website logs, and third-party sources.", "source": "1-introduction"}, {"Q": "What are some benefits of a lakehouse?", "A": "Some benefits of a lakehouse include using Spark and SQL engines to process large-scale data, support for ACID transactions, and having a single source for data engineers, scientists, and analysts.", "source": "2-fabric-lakehouse"}, {"Q": "What's a schema-on-read format?", "A": "A schema-on-read format means that the schema is defined as needed, rather than having a predefined schema.", "source": "2-fabric-lakehouse"}, {"Q": "What can you do with a lakehouse?", "A": "Lakehouse data supports SQL queries, machine learning models, real-time analytics, and Power BI reports.", "source": "2-fabric-lakehouse"}, {"Q": "What are the two modes in which you can work with data in a lakehouse?", "A": "Lakehouse and SQL analytics endpoint.", "source": "3-work-lakehouse"}, {"Q": "What are shortcuts in Fabric used for?", "A": "To integrate data into the lakehouse while keeping it stored in external storage.", "source": "3-work-lakehouse"}, {"Q": "How can you explore data in lakehouse tables?", "A": "You can use the SQL endpoint to run Transact-SQL statements to query and explore data.", "source": "4-explore-data-lakehouse"}, {"Q": "What are Dataflows (Gen2)?", "A": "Dataflows are used to perform transformations through Power Query and land the transformed data back to the lakehouse.", "source": "4-explore-data-lakehouse"}, {"Q": "What are data pipelines?", "A": "Data pipelines are used to orchestrate complex data transformation logic on data in the lakehouse using activities like dataflows and Spark jobs.", "source": "4-explore-data-lakehouse"}], "describe-medallion-architecture": [{"Q": "What's the medallion architecture?", "A": "The medallion architecture is a recommended data design pattern used to organize data in a lakehouse logically.", "source": "2-describe-medallion-architecture"}, {"Q": "What are the three layers of the medallion architecture?", "A": "The three layers are bronze (raw), silver (validated), and gold (enriched), each representing higher data quality levels.", "source": "2-describe-medallion-architecture"}, {"Q": "What's the purpose of the bronze layer?", "A": "The bronze layer is the landing zone for all data, where no changes have been made to it.", "source": "2-describe-medallion-architecture"}, {"Q": "What activities are typically done in the silver layer?", "A": "Activities in the silver layer include combining and merging data, enforcing data validation rules, and cleaning the data.", "source": "2-describe-medallion-architecture"}, {"Q": "What's the purpose of the gold layer?", "A": "The gold layer refines the data further to align with specific business and analytics needs, making it ready for use by downstream teams.", "source": "2-describe-medallion-architecture"}, {"Q": "What's Direct Lake mode in Power BI semantic models?", "A": "It's an approach that combines the performance of a semantic model with the freshness of lakehouse data.", "source": "4-query-report-data"}, {"Q": "What considerations are involved in designing a CI/CD process for a lakehouse architecture?", "A": "Data quality checks, version control, automated deployments, monitoring, and security measures.", "source": "5-secure-govern"}, {"Q": "Why is CI/CD crucial at the gold layer of a lakehouse?", "A": "CI/CD ensures high-quality, validated, and reliable data for consumption.", "source": "5-secure-govern"}, {"Q": "What are the three layers in the medallion architecture?", "A": "Bronze, Silver, and Gold.", "source": "3-implement-medallion-archecture-fabric"}], "get-started-kusto-fabric": [{"Q": "What is Real-Time Intelligence?", "A": "An end-to-end streaming solution for high-speed data analysis, optimized for time-series data with support for automatic partitioning and indexing of any data format.", "source": "2-define-real-time-analytics"}, {"Q": "What's the benefit of using KQL for data analysis?", "A": "KQL provides an efficient way to find insights and patterns from textual or structured data, especially for data with a time series component.", "source": "1-introduction"}, {"Q": "What are the benefits of using Real-Time Intelligence?", "A": "Real-Time Intelligence enables you to ingest data from any source in any format, run analytical queries directly on raw data, and work with versatile data structures.", "source": "2-define-real-time-analytics"}, {"Q": "What is Kusto Query Language (KQL)?", "A": "Kusto Query Language (KQL) is a declarative query language used to analyze and extract insights from structured, semi-structured, and unstructured data.", "source": "2-define-real-time-analytics"}, {"Q": "What can you do with KQL in Microsoft Fabric?", "A": "With KQL in Microsoft Fabric, you can filter, present, aggregate your data, and quickly paste long, complex queries directly into the editor.", "source": "2-define-real-time-analytics"}, {"Q": "What is a materialized view in a KQL Database?", "A": "A materialized view is a schema entity that stores precomputed results of a query for faster retrieval.", "source": "3-describe-kusto-databases-tables"}]}